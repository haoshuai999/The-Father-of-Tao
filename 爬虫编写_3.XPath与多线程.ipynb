{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. XPath\n",
    "* XPath是一门语言<br/>\n",
    "* XPath可以在XML文档中查找信息<br/>\n",
    "* XPath支持HTML<br/>\n",
    "* XPath可以通过元素和属性来进行导航<br/>\n",
    "* XPath可以用来提取信息<br/>\n",
    "* XPath比正则表达式更加简单、更加厉害"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###安装XPath\n",
    "安装 lxml 模块<br/>\n",
    "from lxml import etree<br/>\n",
    "selector = etree.HTML(源代码)<br/>\n",
    "selector.xpath(自定义)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###XPath与HTML结构\n",
    "* 树状结构<br/>\n",
    "* 逐层展开<br/>\n",
    "* 逐层定位<br/>\n",
    "* 寻找独立节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###基本语法规则\n",
    "//  定位根节点<br/>\n",
    "/  往下层寻找<br/>\n",
    "/text()  提取文本内容<br/>\n",
    "/@XXX  提取属性内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "这是第一条信息\n",
      "这是第二条信息\n",
      "这是第三条信息\n",
      "不需要的信息1\n",
      "不需要的信息1\n",
      "不需要的信息1\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "f = open('D:/Python/crawler/information.html', 'r')\n",
    "html = f.read()\n",
    "f.close()\n",
    "selector = etree.HTML(html)\n",
    "# 不加指定地提取内容\n",
    "content = selector.xpath('//ul/li/text()')\n",
    "print type(content)\n",
    "for each in content:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是第一条信息\n",
      "这是第二条信息\n",
      "这是第三条信息\n"
     ]
    }
   ],
   "source": [
    "# 根据属性提取指定内容\n",
    "content = selector.xpath('//ul[@id=\"useful\"]/li/text()')\n",
    "for each in content:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是第一条信息\n"
     ]
    }
   ],
   "source": [
    "# 进一步进行指定\n",
    "# 和group类似，这里是li[1]而不是li[0]\n",
    "content = selector.xpath('//ul[@id=\"useful\"]/li[1]/text()')\n",
    "print content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.bbc.com/\n",
      "http://www.bbc.com/news/world-us-canada-40590094\n"
     ]
    }
   ],
   "source": [
    "# 提取属性1\n",
    "link = selector.xpath('//a/@href')\n",
    "for each in link:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC新闻\n"
     ]
    }
   ],
   "source": [
    "# 提取属性2\n",
    "title = selector.xpath('//a/@title')[0]\n",
    "print title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###高级运用规则\n",
    "**div[starts-with(@attribute, \"attribute-name\")]**<br/>\n",
    "**div.xpath('string(.)')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要的内容1\n",
      "需要的内容2\n",
      "需要的内容3\n"
     ]
    }
   ],
   "source": [
    "f = open('D:/Python/crawler/infor2.html', 'r')\n",
    "html = f.read()\n",
    "f.close()\n",
    "selector = etree.HTML(html)\n",
    "content = selector.xpath('//div[starts-with(@id, \"test\")]/text()')\n",
    "for each in content:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我左青龙，\n",
      "我左青龙，右白虎，上朱雀，下玄武。老牛在当中，龙头在胸口。\n"
     ]
    }
   ],
   "source": [
    "f = open('D:/Python/crawler/infor3.html', 'r')\n",
    "html = f.read()\n",
    "f.close()\n",
    "selector = etree.HTML(html)\n",
    "text_field = selector.xpath('//body/div[@id=\"test3\"]/text()')[0]\n",
    "print text_field.replace('\\n', '').replace(' ', '')\n",
    "info = selector.xpath('//div[@id=\"test3\"]')[0].xpath('string(.)')\n",
    "content = info.replace('\\n', '').replace(' ', '')\n",
    "print content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Python多线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###特点：\n",
    "多个线程同时处理任务<br/>\n",
    "高效<br/>\n",
    "快速"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###map函数的使用：\n",
    "map函数包含序列操作、参数传递、结果保存等一系列操作<br/>\n",
    "from multipricessing.dummy import Pool<br/>\n",
    "pool = Pool(4)<br/>\n",
    "results = pool.map(爬取函数，目标网址列表)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 引申：map 与 filter<br/>\n",
    "> map会将一个函数映射到一个输入列表的所有元素上<br/>\n",
    "> filter进行筛选<br/>\n",
    "> 两者都要结合lambda<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "items = [1, 2, 3, 4, 5]\n",
    "s = []\n",
    "for i in items:\n",
    "    s.append(i ** 2)\n",
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "s_1 = list(map(lambda x : x ** 2, items))\n",
    "print s_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[1, 2]\n",
      "[4, 4]\n",
      "[9, 6]\n",
      "[16, 8]\n"
     ]
    }
   ],
   "source": [
    "# 不仅用于一列表的所有元素输入，甚至可以用于一列表的函数\n",
    "def mul(x):\n",
    "    return x * x\n",
    "def add(x):\n",
    "    return x + x\n",
    "funcs = [mul, add]\n",
    "for i  in range(5):\n",
    "    value = list(map(lambda x : x(i), funcs))\n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = range(-5, 5)\n",
    "less = list(filter(lambda x : x > 0, num))\n",
    "less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###多线程爬虫原理\n",
    "* requests获取网页<br/>\n",
    "* XPath提取内容<br/>\n",
    "* map实现多线程<br/>\n",
    "<br/>\n",
    "* 提示：如果出现中文乱码，建议先把保存在本地的文件用Notepad++打开，在“格式”中选择**“以UTF-8格式编码”**，保存后执行可恢复正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from lxml import etree\n",
    "import json\n",
    "\n",
    "html = open('D:/Python/crawler/tieba.html', 'r').read()\n",
    "selector = etree.HTML(html)\n",
    "content_field = selector.xpath('//ul/li[@class=\"j_thread_list clearfix\"]')\n",
    "for each in content_field:\n",
    "    data_field = json.loads(each.xpath('@data-field')[0].replace('&quot', ''))\n",
    "    author_name = data_field['author_name']\n",
    "    user_id = data_field['id']\n",
    "    reply_num = data_field['reply_num']\n",
    "    title = each.xpath('div[@class=\"t_con cleafix\"]/div[@class=\"threadlist_li_right j_threadlist_li_right \"]/div\\\n",
    "    [@class=\"threadlist_lz clearfix\"]/div[@class=\"threadlist_title pull_left j_th_tit\"]/a/text()')\n",
    "    time = each.xpath('div[@class=\"t_con cleafix\"]/div[@class=\"threadlist_li_right j_threadlist_li_right \"]/div\\\n",
    "    [@class=\"threadlist_detail clearfix\"]/div[@class=\"threadlist_author\"]/span[@class=\"threadlist_reply_date pull_right j_reply_data\"]/text()')[0].lstrip(' ')\n",
    "    if len(title) > 0:\n",
    "        title_content = title[0]\n",
    "    else:\n",
    "        title_content = ' '\n",
    "    print str(author_name.encode('utf-8')) + ', ' + str(user_id) + ', ' + str(reply_num) + ', ' + str(title_content.encode('utf-8')) + ', ' + str(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在抓取……\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import requests, json\n",
    "\n",
    "class spider(object):\n",
    "    def __init__(self):\n",
    "        print '正在抓取……'\n",
    "    \n",
    "    def true_spider(self, url):\n",
    "        html = requests.get(url)\n",
    "        selector = etree.HTML(html.text)\n",
    "        content_field = selector.xpath('//li[@class=\"j_thread_list clearfix\"]')\n",
    "        for each in content_field:\n",
    "            data_field = json.loads(each.xpath('@data-field')[0].replace('&quot', ''))\n",
    "            author_name = data_field['author_name']\n",
    "            user_id = data_field['id']\n",
    "            reply_num = data_field['reply_num']\n",
    "            if each.xpath('div[@class=\"t_con cleafix\"]/div[@class=\"threadlist_li_right j_threadlist_li_right \"]/div\\\n",
    "            [@class=\"threadlist_lz clearfix\"]/div/@class')[0] == 'threadlist_text threadlist_title j_th_tit  ':\n",
    "                title = each.xpath('div[@class=\"t_con cleafix\"]/div[@class=\"threadlist_li_right j_threadlist_li_right \"]/div\\\n",
    "                [@class=\"threadlist_lz clearfix\"]/div[@class=\"threadlist_text threadlist_title j_th_tit  \"]/a/@title')[0]\n",
    "                time =each.xpath('div[@class=\"t_con cleafix\"]/div[@class=\"threadlist_li_right j_threadlist_li_right \"]/div\\\n",
    "                [@class=\"threadlist_detail clearfix\"]/div[@class=\"threadlist_author\"]/span[@class=\"threadlist_reply_date j_reply_data\"]/text()')\n",
    "                if len(time) == 0:\n",
    "                    time = ' '\n",
    "                else:\n",
    "                    time = time[0].lstrip(' ')\n",
    "            else:\n",
    "                title = each.xpath('div[@class=\"t_con cleafix\"]/div[@class=\"threadlist_li_right j_threadlist_li_right \"]/div\\\n",
    "                [@class=\"threadlist_lz clearfix\"]/div[@class=\"threadlist_text threadlist_title j_th_tit  member_thread_title_frs  \"]/a/@title')[0]\n",
    "                time = each.xpath('div[@class=\"t_con cleafix\"]/div[@class=\"threadlist_li_right j_threadlist_li_right \"]/div[@class=\"threadlist_detail clearfix\"]/div\\\n",
    "                [@class=\"threadlist_author\"]/span[@class=\"threadlist_reply_date j_reply_data\"]/text()')\n",
    "                if len(time) == 0:\n",
    "                    time = ' '\n",
    "                else:\n",
    "                    time = time[0].lstrip(' ')\n",
    "            print str(author_name.encode('utf-8')) + ', ' + str(user_id) + ', ' + str(reply_num) + ', ' + str(title.encode('utf-8')) + ', ' + str(time)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tiebaspider = spider()\n",
    "    for i in range(0, 50, 50):\n",
    "        new_page = 'http://tieba.baidu.com/f?kw=python&ie=utf-8&pn=' + str(i)\n",
    "        tiebaspider.true_spider(new_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single thread costs: 5.72699999809\n",
      "Multi threads costs: 1.60899996758\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import requests, time\n",
    "\n",
    "def getsource(url):\n",
    "    html = requests.get(url)\n",
    "urls = []\n",
    "for i in range(1, 11):\n",
    "    newpage = 'http://tieba.baidu.com/p/3522395718?pn=' + str(i)\n",
    "    urls.append(newpage)\n",
    "time1 = time.time()\n",
    "for i in urls:\n",
    "    getsource(i)\n",
    "time2 = time.time()\n",
    "print 'Single thread costs: ' + str(time2 - time1)\n",
    "\n",
    "pool = ThreadPool(4)\n",
    "time3 = time.time()\n",
    "results = pool.map(getsource, urls)\n",
    "pool.close()\n",
    "pool.join()\n",
    "time4 = time.time()\n",
    "print 'Multi threads costs: ' + str(time4 - time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
